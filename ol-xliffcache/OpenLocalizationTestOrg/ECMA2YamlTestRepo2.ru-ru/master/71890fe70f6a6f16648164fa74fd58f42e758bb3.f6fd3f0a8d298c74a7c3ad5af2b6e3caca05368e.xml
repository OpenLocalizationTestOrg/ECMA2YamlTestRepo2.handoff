{"nodes":[{"pos":[752,866],"content":"Represents audio input that is associated with a <bpt id=\"p1\">&lt;xref href=\"System.Speech.Recognition.RecognitionResult\"&gt;</bpt><ept id=\"p1\">&lt;/xref&gt;</ept>.","needQuote":true,"needEscape":true,"source":"Represents audio input that is associated with a <xref href=\"System.Speech.Recognition.RecognitionResult\"></xref>."},{"pos":[879,3745],"content":"A speech recognizer generates information about the audio input as part of the recognition operation. To access the recognized audio, use the <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> property or the <xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> method of the <xref:System.Speech.Recognition.RecognitionResult>.       A recognition result can be produced by the following events and methods of the <xref:System.Speech.Recognition.SpeechRecognizer> and <xref:System.Speech.Recognition.SpeechRecognitionEngine> classes:      -   Events:          -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized?displayProperty=fullName>          -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected?displayProperty=fullName>          -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName>          -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted?displayProperty=fullName>          -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted?displayProperty=fullName>      -   Methods:          -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A?displayProperty=fullName>          -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A?displayProperty=fullName>          -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A?displayProperty=fullName>          -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A?displayProperty=fullName>      > [!IMPORTANT] >  A recognition result produced by emulated speech recognition does not contain recognized audio. For such a recognition result, its <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> property returns `null` and its <xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> method throws an exception. For more information about emulated speech recognition, see the `EmulateRecognize` and `EmulateRecognizeAsync` methods of the <xref:System.Speech.Recognition.SpeechRecognizer> and <xref:System.Speech.Recognition.SpeechRecognitionEngine> classes.","needQuote":false,"needEscape":true,"extradata":"MT","nodes":[{"content":"A speech recognizer generates information about the audio input as part of the recognition operation.","pos":[0,101]},{"content":"To access the recognized audio, use the &lt;xref:System.Speech.Recognition.RecognitionResult.Audio%2A&gt; property or the &lt;xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A&gt; method of the &lt;xref:System.Speech.Recognition.RecognitionResult&gt;.","pos":[102,358],"source":" To access the recognized audio, use the <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> property or the <xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> method of the <xref:System.Speech.Recognition.RecognitionResult>."},{"content":"A recognition result can be produced by the following events and methods of the &lt;xref:System.Speech.Recognition.SpeechRecognizer&gt; and &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt; classes:      -   Events:          -   &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized?displayProperty=fullName&gt; and &lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized?displayProperty=fullName&gt;          -   &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected?displayProperty=fullName&gt; and &lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected?displayProperty=fullName&gt;          -   &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName&gt; and &lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName&gt;          -   &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted?displayProperty=fullName&gt; and &lt;xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted?displayProperty=fullName&gt;          -   &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted?displayProperty=fullName&gt;      -   Methods:          -   &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A?displayProperty=fullName&gt; and &lt;xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A?displayProperty=fullName&gt;          -   &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A?displayProperty=fullName&gt; and &lt;xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A?displayProperty=fullName&gt;          -   &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A?displayProperty=fullName&gt;          -   &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A?displayProperty=fullName&gt;      &gt; <ph id=\"ph1\">[!IMPORTANT]</ph> &gt;  A recognition result produced by emulated speech recognition does not contain recognized audio.","pos":[365,2363],"source":"       A recognition result can be produced by the following events and methods of the <xref:System.Speech.Recognition.SpeechRecognizer> and <xref:System.Speech.Recognition.SpeechRecognitionEngine> classes:      -   Events:          -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized?displayProperty=fullName>          -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected?displayProperty=fullName>          -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName>          -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted?displayProperty=fullName>          -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted?displayProperty=fullName>      -   Methods:          -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A?displayProperty=fullName>          -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A?displayProperty=fullName>          -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A?displayProperty=fullName>          -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A?displayProperty=fullName>      > [!IMPORTANT] >  A recognition result produced by emulated speech recognition does not contain recognized audio."},{"content":"For such a recognition result, its &lt;xref:System.Speech.Recognition.RecognitionResult.Audio%2A&gt; property returns <ph id=\"ph1\">`null`</ph> and its &lt;xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A&gt; method throws an exception.","pos":[2364,2593],"source":" For such a recognition result, its <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> property returns `null` and its <xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> method throws an exception."},{"content":"For more information about emulated speech recognition, see the <ph id=\"ph1\">`EmulateRecognize`</ph> and <ph id=\"ph2\">`EmulateRecognizeAsync`</ph> methods of the &lt;xref:System.Speech.Recognition.SpeechRecognizer&gt; and &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt; classes.","pos":[2594,2839],"source":" For more information about emulated speech recognition, see the `EmulateRecognize` and `EmulateRecognizeAsync` methods of the <xref:System.Speech.Recognition.SpeechRecognizer> and <xref:System.Speech.Recognition.SpeechRecognitionEngine> classes."}]},{"pos":[5463,5545],"content":"Gets the location in the input audio stream for the start of the recognized audio.","needQuote":true,"needEscape":true,"nodes":[{"content":"Gets the location in the input audio stream for the start of the recognized audio.","pos":[0,82]}]},{"pos":[5558,6233],"content":"This property references the position at the beginning of the recognized phrase in the input device's generated audio stream. By contrast, the `RecognizerAudioPosition` property of the <xref:System.Speech.Recognition.SpeechRecognitionEngine> and <xref:System.Speech.Recognition.SpeechRecognizer> classes reference the recognizer's position within its audio input. These positions can be different. For more information, see [Using Speech Recognition Events](http://msdn.microsoft.com/en-us/01c598ca-2e0e-4e89-b303-cd1cef9e8482).       The <xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A> property gets the system time at the start of the recognition operation.","needQuote":false,"needEscape":true,"extradata":"MT","nodes":[{"content":"This property references the position at the beginning of the recognized phrase in the input device's generated audio stream.","pos":[0,125]},{"content":"By contrast, the <ph id=\"ph1\">`RecognizerAudioPosition`</ph> property of the &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt; and &lt;xref:System.Speech.Recognition.SpeechRecognizer&gt; classes reference the recognizer's position within its audio input.","pos":[126,363],"source":" By contrast, the `RecognizerAudioPosition` property of the <xref:System.Speech.Recognition.SpeechRecognitionEngine> and <xref:System.Speech.Recognition.SpeechRecognizer> classes reference the recognizer's position within its audio input."},{"content":"These positions can be different.","pos":[364,397]},{"content":"For more information, see <bpt id=\"p1\">[</bpt>Using Speech Recognition Events<ept id=\"p1\">](http://msdn.microsoft.com/en-us/01c598ca-2e0e-4e89-b303-cd1cef9e8482)</ept>.","pos":[398,528],"source":" For more information, see [Using Speech Recognition Events](http://msdn.microsoft.com/en-us/01c598ca-2e0e-4e89-b303-cd1cef9e8482)."},{"content":"The &lt;xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A&gt; property gets the system time at the start of the recognition operation.","pos":[535,673],"source":"       The <xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A> property gets the system time at the start of the recognition operation."}]},{"pos":[7449,7526],"content":"The location in the input audio stream for the start of the recognized audio.","needQuote":true,"needEscape":true,"nodes":[{"content":"The location in the input audio stream for the start of the recognized audio.","pos":[0,77]}]},{"pos":[8001,8070],"content":"Gets the duration of the input audio stream for the recognized audio.","needQuote":true,"needEscape":true,"nodes":[{"content":"Gets the duration of the input audio stream for the recognized audio.","pos":[0,69]}]},{"pos":[9294,9362],"content":"The duration within the input audio stream for the recognized audio.","needQuote":true,"needEscape":true,"nodes":[{"content":"The duration within the input audio stream for the recognized audio.","pos":[0,68]}]},{"pos":[9822,9885],"content":"Gets the format of the audio processed by a recognition engine.","needQuote":true,"needEscape":true,"nodes":[{"content":"Gets the format of the audio processed by a recognition engine.","pos":[0,63]}]},{"pos":[11178,11237],"content":"The format of the audio processed by the speech recognizer.","needQuote":true,"needEscape":true,"nodes":[{"content":"The format of the audio processed by the speech recognizer.","pos":[0,59]}]},{"pos":[11826,11903],"content":"Selects and returns a section of the current recognized audio as binary data.","needQuote":true,"needEscape":true,"nodes":[{"content":"Selects and returns a section of the current recognized audio as binary data.","pos":[0,77]}]},{"pos":[14909,14961],"content":"The starting point of the audio data to be returned.","needQuote":true,"needEscape":true,"nodes":[{"content":"The starting point of the audio data to be returned.","pos":[0,52]}]},{"pos":[15028,15069],"content":"The length of the segment to be returned.","needQuote":true,"needEscape":true,"nodes":[{"content":"The length of the segment to be returned.","pos":[0,41]}]},{"pos":[15155,15268],"content":"Returns a subsection of the recognized audio, as defined by <bpt id=\"p1\">&lt;code&gt;</bpt><ph id=\"ph1\">audioPosition</ph><ept id=\"p1\">&lt;/code&gt;</ept> and <bpt id=\"p2\">&lt;code&gt;</bpt><ph id=\"ph2\">duration</ph><ept id=\"p2\">&lt;/code&gt;</ept>.","needQuote":true,"needEscape":true,"source":"Returns a subsection of the recognized audio, as defined by <code>audioPosition</code> and <code>duration</code>."},{"pos":[15461,15581],"content":"<ph id=\"ph1\">&lt;code&gt;audioPosition&lt;/code&gt;</ph> and <ph id=\"ph2\">&lt;code&gt;duration&lt;/code&gt;</ph> define a segment of audio outside the range of the current segment.","needQuote":true,"needEscape":true,"source":"<code>audioPosition</code> and <code>duration</code> define a segment of audio outside the range of the current segment."},{"pos":[15692,15738],"content":"The current recognized audio contains no data.","needQuote":true,"needEscape":true,"nodes":[{"content":"The current recognized audio contains no data.","pos":[0,46]}]},{"pos":[16132,16195],"content":"Gets the system time at the start of the recognition operation.","needQuote":true,"needEscape":true,"nodes":[{"content":"Gets the system time at the start of the recognition operation.","pos":[0,63]}]},{"pos":[16208,16507],"content":"The StartTime property gets the system time at the start of the recognition operation, which can be useful for latency and performance calculations.       The <xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A> property gets the location in the input device's generated audio stream.","needQuote":false,"needEscape":true,"extradata":"MT","nodes":[{"content":"The StartTime property gets the system time at the start of the recognition operation, which can be useful for latency and performance calculations.       The <xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A> property gets the location in the input device's generated audio stream.","pos":[0,297],"nodes":[{"content":"The StartTime property gets the system time at the start of the recognition operation, which can be useful for latency and performance calculations.","pos":[0,148]},{"content":"The &lt;xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A&gt; property gets the location in the input device's generated audio stream.","pos":[155,297],"source":"       The <xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A> property gets the location in the input device's generated audio stream."}]}]},{"pos":[17719,17777],"content":"The system time at the start of the recognition operation.","needQuote":true,"needEscape":true,"nodes":[{"content":"The system time at the start of the recognition operation.","pos":[0,58]}]},{"pos":[18356,18404],"content":"Writes the entire audio to a stream as raw data.","needQuote":true,"needEscape":true,"nodes":[{"content":"Writes the entire audio to a stream as raw data.","pos":[0,48]}]},{"pos":[18417,18719],"content":"Audio data is written to `outputStream` in binary form. No header information is included.       The WriteToAudioStream method uses the Wave format, but does not include the Wave header. To include the Wave header, use the <xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A> method.","needQuote":false,"needEscape":true,"extradata":"MT","nodes":[{"content":"Audio data is written to <ph id=\"ph1\">`outputStream`</ph> in binary form.","pos":[0,55],"source":"Audio data is written to `outputStream` in binary form."},{"content":"No header information is included.","pos":[56,90]},{"content":"The WriteToAudioStream method uses the Wave format, but does not include the Wave header.","pos":[97,186]},{"content":"To include the Wave header, use the &lt;xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A&gt; method.","pos":[187,300],"source":" To include the Wave header, use the <xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A> method."}]},{"pos":[18895,18939],"content":"The stream that will receive the audio data.","needQuote":true,"needEscape":true,"nodes":[{"content":"The stream that will receive the audio data.","pos":[0,44]}]},{"pos":[19522,19562],"content":"Writes audio to a stream in Wave format.","needQuote":true,"needEscape":true,"nodes":[{"content":"Writes audio to a stream in Wave format.","pos":[0,40]}]},{"pos":[19575,19852],"content":"Audio data is written to `outputStream` in Wave format, which includes a resource interchange file format (RIFF) header.       The <xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A> method uses the same binary format, but does not include the Wave header.","needQuote":false,"needEscape":true,"extradata":"MT","nodes":[{"content":"Audio data is written to <ph id=\"ph1\">`outputStream`</ph> in Wave format, which includes a resource interchange file format (RIFF) header.","pos":[0,120],"source":"Audio data is written to `outputStream` in Wave format, which includes a resource interchange file format (RIFF) header."},{"content":"The &lt;xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A&gt; method uses the same binary format, but does not include the Wave header.","pos":[127,275],"source":"       The <xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A> method uses the same binary format, but does not include the Wave header."}]},{"pos":[22805,22849],"content":"The stream that will receive the audio data.","needQuote":true,"needEscape":true,"nodes":[{"content":"The stream that will receive the audio data.","pos":[0,44]}]}],"content":"### YamlMime:ManagedReference\nitems:\n- uid: System.Speech.Recognition.RecognizedAudio\n  id: RecognizedAudio\n  children:\n  - System.Speech.Recognition.RecognizedAudio.AudioPosition\n  - System.Speech.Recognition.RecognizedAudio.Duration\n  - System.Speech.Recognition.RecognizedAudio.Format\n  - System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)\n  - System.Speech.Recognition.RecognizedAudio.StartTime\n  - System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)\n  - System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)\n  langs:\n  - csharp\n  name: RecognizedAudio\n  nameWithType: RecognizedAudio\n  fullName: System.Speech.Recognition.RecognizedAudio\n  type: Class\n  summary: Represents audio input that is associated with a <xref href=\"System.Speech.Recognition.RecognitionResult\"></xref>.\n  remarks: \"A speech recognizer generates information about the audio input as part of the recognition operation. To access the recognized audio, use the <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> property or the <xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> method of the <xref:System.Speech.Recognition.RecognitionResult>.  \\n  \\n A recognition result can be produced by the following events and methods of the <xref:System.Speech.Recognition.SpeechRecognizer> and <xref:System.Speech.Recognition.SpeechRecognitionEngine> classes:  \\n  \\n-   Events:  \\n  \\n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized?displayProperty=fullName>  \\n  \\n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected?displayProperty=fullName>  \\n  \\n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName>  \\n  \\n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted?displayProperty=fullName>  \\n  \\n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted?displayProperty=fullName>  \\n  \\n-   Methods:  \\n  \\n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A?displayProperty=fullName>  \\n  \\n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A?displayProperty=fullName> and <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A?displayProperty=fullName>  \\n  \\n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A?displayProperty=fullName>  \\n  \\n    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A?displayProperty=fullName>  \\n  \\n> [!IMPORTANT]\\n>  A recognition result produced by emulated speech recognition does not contain recognized audio. For such a recognition result, its <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> property returns `null` and its <xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> method throws an exception. For more information about emulated speech recognition, see the `EmulateRecognize` and `EmulateRecognizeAsync` methods of the <xref:System.Speech.Recognition.SpeechRecognizer> and <xref:System.Speech.Recognition.SpeechRecognitionEngine> classes.\"\n  example:\n  - \"The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName>, or <xref:System.Speech.Recognition.Grammar.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \\n  \\n```c#  \\n  \\n// Handle the SpeechRecognized event.   \\nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \\n{  \\n  if (e.Result == null) return;  \\n  \\n  RecognitionResult result = e.Result;  \\n  \\n  Console.WriteLine(\\\"Grammar({0}): {1}\\\",  \\n    result.Grammar.Name, result.Text);  \\n  \\n  if (e.Result.Audio != null)  \\n  {  \\n    RecognizedAudio audio = e.Result.Audio;  \\n  \\n    Console.WriteLine(\\\"   start time: {0}\\\", audio.StartTime);  \\n    Console.WriteLine(\\\"   encoding format: {0}\\\", audio.Format.EncodingFormat);  \\n    Console.WriteLine(\\\"   position: {0}, duration: {1}\\\",  \\n      audio.AudioPosition, audio.Duration);  \\n  }  \\n  \\n  // Add event handler code here.  \\n}  \\n```\"\n  syntax:\n    content: public class RecognizedAudio\n  inheritance:\n  - System.Object\n  implements: []\n  inheritedMembers: []\n  platform:\n  - net462\n- uid: System.Speech.Recognition.RecognizedAudio.AudioPosition\n  id: AudioPosition\n  parent: System.Speech.Recognition.RecognizedAudio\n  langs:\n  - csharp\n  name: AudioPosition\n  nameWithType: RecognizedAudio.AudioPosition\n  fullName: System.Speech.Recognition.RecognizedAudio.AudioPosition\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets the location in the input audio stream for the start of the recognized audio.\n  remarks: \"This property references the position at the beginning of the recognized phrase in the input device's generated audio stream. By contrast, the `RecognizerAudioPosition` property of the <xref:System.Speech.Recognition.SpeechRecognitionEngine> and <xref:System.Speech.Recognition.SpeechRecognizer> classes reference the recognizer's position within its audio input. These positions can be different. For more information, see [Using Speech Recognition Events](http://msdn.microsoft.com/en-us/01c598ca-2e0e-4e89-b303-cd1cef9e8482).  \\n  \\n The <xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A> property gets the system time at the start of the recognition operation.\"\n  example:\n  - \"The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> or <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \\n  \\n```c#  \\n  \\n// Handle the SpeechRecognized event.   \\nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \\n{  \\n  if (e.Result == null) return;  \\n  \\n  RecognitionResult result = e.Result;  \\n  \\n  Console.WriteLine(\\\"Grammar({0}): {1}\\\",  \\n    result.Grammar.Name, result.Text);  \\n  \\n  if (e.Result.Audio != null)  \\n  {  \\n    RecognizedAudio audio = e.Result.Audio;  \\n  \\n    Console.WriteLine(\\\"   start time: {0}\\\", audio.StartTime);  \\n    Console.WriteLine(\\\"   encoding format: {0}\\\", audio.Format.EncodingFormat);  \\n    Console.WriteLine(\\\"   position: {0}, duration: {1}\\\",  \\n      audio.AudioPosition, audio.Duration);  \\n  }  \\n  \\n  // Add event handler code here.  \\n}  \\n```\"\n  syntax:\n    content: public TimeSpan AudioPosition { get; }\n    return:\n      type: System.TimeSpan\n      description: The location in the input audio stream for the start of the recognized audio.\n  overload: System.Speech.Recognition.RecognizedAudio.AudioPosition*\n  exceptions: []\n  platform:\n  - net462\n- uid: System.Speech.Recognition.RecognizedAudio.Duration\n  id: Duration\n  parent: System.Speech.Recognition.RecognizedAudio\n  langs:\n  - csharp\n  name: Duration\n  nameWithType: RecognizedAudio.Duration\n  fullName: System.Speech.Recognition.RecognizedAudio.Duration\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets the duration of the input audio stream for the recognized audio.\n  remarks: ''\n  example:\n  - \"The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> or <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \\n  \\n```c#  \\n  \\n// Handle the SpeechRecognized event.   \\nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \\n{  \\n  if (e.Result == null) return;  \\n  \\n  RecognitionResult result = e.Result;  \\n  \\n  Console.WriteLine(\\\"Grammar({0}): {1}\\\",  \\n    result.Grammar.Name, result.Text);  \\n  \\n  if (e.Result.Audio != null)  \\n  {  \\n    RecognizedAudio audio = e.Result.Audio;  \\n  \\n    Console.WriteLine(\\\"   start time: {0}\\\", audio.StartTime);  \\n    Console.WriteLine(\\\"   encoding format: {0}\\\", audio.Format.EncodingFormat);  \\n    Console.WriteLine(\\\"   position: {0}, duration: {1}\\\",  \\n      audio.AudioPosition, audio.Duration);  \\n  }  \\n  \\n  // Add event handler code here.  \\n}  \\n```\"\n  syntax:\n    content: public TimeSpan Duration { get; }\n    return:\n      type: System.TimeSpan\n      description: The duration within the input audio stream for the recognized audio.\n  overload: System.Speech.Recognition.RecognizedAudio.Duration*\n  exceptions: []\n  platform:\n  - net462\n- uid: System.Speech.Recognition.RecognizedAudio.Format\n  id: Format\n  parent: System.Speech.Recognition.RecognizedAudio\n  langs:\n  - csharp\n  name: Format\n  nameWithType: RecognizedAudio.Format\n  fullName: System.Speech.Recognition.RecognizedAudio.Format\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets the format of the audio processed by a recognition engine.\n  remarks: ''\n  example:\n  - \"The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> or <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \\n  \\n```c#  \\n  \\n// Handle the SpeechRecognized event.   \\nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \\n{  \\n  if (e.Result == null) return;  \\n  \\n  RecognitionResult result = e.Result;  \\n  \\n  Console.WriteLine(\\\"Grammar({0}): {1}\\\",  \\n    result.Grammar.Name, result.Text);  \\n  \\n  if (e.Result.Audio != null)  \\n  {  \\n    RecognizedAudio audio = e.Result.Audio;  \\n  \\n    Console.WriteLine(\\\"   start time: {0}\\\", audio.StartTime);  \\n    Console.WriteLine(\\\"   encoding format: {0}\\\", audio.Format.EncodingFormat);  \\n    Console.WriteLine(\\\"   position: {0}, duration: {1}\\\",  \\n      audio.AudioPosition, audio.Duration);  \\n  }  \\n  \\n  // Add event handler code here.  \\n}  \\n```\"\n  syntax:\n    content: public System.Speech.AudioFormat.SpeechAudioFormatInfo Format { get; }\n    return:\n      type: System.Speech.AudioFormat.SpeechAudioFormatInfo\n      description: The format of the audio processed by the speech recognizer.\n  overload: System.Speech.Recognition.RecognizedAudio.Format*\n  exceptions: []\n  platform:\n  - net462\n- uid: System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)\n  id: GetRange(System.TimeSpan,System.TimeSpan)\n  parent: System.Speech.Recognition.RecognizedAudio\n  langs:\n  - csharp\n  name: GetRange(TimeSpan,TimeSpan)\n  nameWithType: RecognizedAudio.GetRange(TimeSpan,TimeSpan)\n  fullName: System.Speech.Recognition.RecognizedAudio.GetRange(TimeSpan,TimeSpan)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Selects and returns a section of the current recognized audio as binary data.\n  remarks: ''\n  example:\n  - \"The following example creates a speech recognition grammar for name input, adds a handler for the <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event, and loads the grammar into an in-process speech recognizer. Then it writes the audio information for the name portion of the input to an audio file. The audio file is used as input to a <xref:System.Speech.Synthesis.SpeechSynthesizer> object, which speaks a phrase that includes the recorded audio.  \\n  \\n```  \\nprivate static void AddNameGrammar(SpeechRecognitionEngine recognizer)  \\n{  \\n  GrammarBuilder builder = new GrammarBuilder();  \\n  builder.Append(\\\"My name is\\\");  \\n  builder.AppendWildcard();  \\n  \\n  Grammar nameGrammar = new Grammar(builder);  \\n  nameGrammar.Name = \\\"Name Grammar\\\";  \\n  nameGrammar.SpeechRecognized +=  \\n    new EventHandler<SpeechRecognizedEventArgs>(  \\n      NameSpeechRecognized);  \\n  \\n  recognizer.LoadGrammar(nameGrammar);  \\n}  \\n  \\n// Handle the SpeechRecognized event of the name grammar.  \\nprivate static void NameSpeechRecognized(  \\n  object sender, SpeechRecognizedEventArgs e)  \\n{  \\n  Console.WriteLine(\\\"Grammar ({0}) recognized speech: {1}\\\",  \\n    e.Result.Grammar.Name, e.Result.Text);  \\n  \\n  try  \\n  {  \\n  \\n    // The name phrase starts after the first three words.  \\n    if (e.Result.Words.Count < 4)  \\n    {  \\n  \\n      // Add code to check for an alternate that contains the wildcard.  \\n      return;  \\n    }  \\n  \\n    RecognizedAudio audio = e.Result.Audio;  \\n    TimeSpan start = e.Result.Words[3].AudioPosition;  \\n    TimeSpan duration = audio.Duration - start;  \\n  \\n    // Add code to verify and persist the audio.  \\n    string path = @\\\"C:\\\\temp\\\\nameAudio.wav\\\";  \\n    using (Stream outputStream = new FileStream(path, FileMode.Create))  \\n    {  \\n      RecognizedAudio nameAudio = audio.GetRange(start, duration);  \\n      nameAudio.WriteToWaveStream(outputStream);  \\n      outputStream.Close();  \\n    }  \\n  \\n    Thread testThread =  \\n      new Thread(new ParameterizedThreadStart(TestAudio));  \\n    testThread.Start(path);  \\n  }  \\n  catch (Exception ex)  \\n  {  \\n    Console.WriteLine(\\\"Exception thrown while processing audio:\\\");  \\n    Console.WriteLine(ex.ToString());  \\n  }  \\n}  \\n  \\n// Use the speech synthesizer to play back the .wav file  \\n// that was created in the SpeechRecognized event handler.  \\n  \\nprivate static void TestAudio(object item)  \\n{  \\n  string path = item as string;  \\n  if (path != null && File.Exists(path))  \\n  {  \\n    SpeechSynthesizer synthesizer = new SpeechSynthesizer();  \\n    PromptBuilder builder = new PromptBuilder();  \\n    builder.AppendText(\\\"Hello\\\");  \\n    builder.AppendAudio(path);  \\n    synthesizer.Speak(builder);  \\n  }  \\n}  \\n```\"\n  syntax:\n    content: public System.Speech.Recognition.RecognizedAudio GetRange (TimeSpan audioPosition, TimeSpan duration);\n    parameters:\n    - id: audioPosition\n      type: System.TimeSpan\n      description: The starting point of the audio data to be returned.\n    - id: duration\n      type: System.TimeSpan\n      description: The length of the segment to be returned.\n    return:\n      type: System.Speech.Recognition.RecognizedAudio\n      description: Returns a subsection of the recognized audio, as defined by <code>audioPosition</code> and <code>duration</code>.\n  overload: System.Speech.Recognition.RecognizedAudio.GetRange*\n  exceptions:\n  - type: System.ArgumentOutOfRangeException\n    commentId: T:System.ArgumentOutOfRangeException\n    description: <code>audioPosition</code> and <code>duration</code> define a segment of audio outside the range of the current segment.\n  - type: System.InvalidOperationException\n    commentId: T:System.InvalidOperationException\n    description: The current recognized audio contains no data.\n  platform:\n  - net462\n- uid: System.Speech.Recognition.RecognizedAudio.StartTime\n  id: StartTime\n  parent: System.Speech.Recognition.RecognizedAudio\n  langs:\n  - csharp\n  name: StartTime\n  nameWithType: RecognizedAudio.StartTime\n  fullName: System.Speech.Recognition.RecognizedAudio.StartTime\n  type: Property\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Gets the system time at the start of the recognition operation.\n  remarks: \"The StartTime property gets the system time at the start of the recognition operation, which can be useful for latency and performance calculations.  \\n  \\n The <xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A> property gets the location in the input device's generated audio stream.\"\n  example:\n  - \"The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> or <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \\n  \\n```c#  \\n  \\n// Handle the SpeechRecognized event.   \\nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \\n{  \\n  if (e.Result == null) return;  \\n  \\n  RecognitionResult result = e.Result;  \\n  \\n  Console.WriteLine(\\\"Grammar({0}): {1}\\\",  \\n    result.Grammar.Name, result.Text);  \\n  \\n  if (e.Result.Audio != null)  \\n  {  \\n    RecognizedAudio audio = e.Result.Audio;  \\n  \\n    Console.WriteLine(\\\"   start time: {0}\\\", audio.StartTime);  \\n    Console.WriteLine(\\\"   encoding format: {0}\\\", audio.Format.EncodingFormat);  \\n    Console.WriteLine(\\\"   position: {0}, duration: {1}\\\",  \\n      audio.AudioPosition, audio.Duration);  \\n  }  \\n  \\n  // Add event handler code here.  \\n}  \\n```\"\n  syntax:\n    content: public DateTime StartTime { get; }\n    return:\n      type: System.DateTime\n      description: The system time at the start of the recognition operation.\n  overload: System.Speech.Recognition.RecognizedAudio.StartTime*\n  exceptions: []\n  platform:\n  - net462\n- uid: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)\n  id: WriteToAudioStream(System.IO.Stream)\n  parent: System.Speech.Recognition.RecognizedAudio\n  langs:\n  - csharp\n  name: WriteToAudioStream(Stream)\n  nameWithType: RecognizedAudio.WriteToAudioStream(Stream)\n  fullName: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(Stream)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Writes the entire audio to a stream as raw data.\n  remarks: \"Audio data is written to `outputStream` in binary form. No header information is included.  \\n  \\n The WriteToAudioStream method uses the Wave format, but does not include the Wave header. To include the Wave header, use the <xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A> method.\"\n  syntax:\n    content: public void WriteToAudioStream (System.IO.Stream outputStream);\n    parameters:\n    - id: outputStream\n      type: System.IO.Stream\n      description: The stream that will receive the audio data.\n  overload: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream*\n  exceptions: []\n  platform:\n  - net462\n- uid: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)\n  id: WriteToWaveStream(System.IO.Stream)\n  parent: System.Speech.Recognition.RecognizedAudio\n  langs:\n  - csharp\n  name: WriteToWaveStream(Stream)\n  nameWithType: RecognizedAudio.WriteToWaveStream(Stream)\n  fullName: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(Stream)\n  type: Method\n  assemblies:\n  - System.Speech\n  namespace: System.Speech.Recognition\n  summary: Writes audio to a stream in Wave format.\n  remarks: \"Audio data is written to `outputStream` in Wave format, which includes a resource interchange file format (RIFF) header.  \\n  \\n The <xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A> method uses the same binary format, but does not include the Wave header.\"\n  example:\n  - \"The following example creates a speech recognition grammar for name input, adds a handler for the <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event, and loads the grammar into an in-process speech recognizer. Then it writes the audio information for the name portion of the input to an audio file. The audio file is used as input to a <xref:System.Speech.Synthesis.SpeechSynthesizer> object, which speaks a phrase that includes the recorded audio.  \\n  \\n```  \\nprivate static void AddNameGrammar(SpeechRecognitionEngine recognizer)  \\n{  \\n  GrammarBuilder builder = new GrammarBuilder();  \\n  builder.Append(\\\"My name is\\\");  \\n  builder.AppendWildcard();  \\n  \\n  Grammar nameGrammar = new Grammar(builder);  \\n  nameGrammar.Name = \\\"Name Grammar\\\";  \\n  nameGrammar.SpeechRecognized +=  \\n    new EventHandler<SpeechRecognizedEventArgs>(  \\n      NameSpeechRecognized);  \\n  \\n  recognizer.LoadGrammar(nameGrammar);  \\n}  \\n  \\n// Handle the SpeechRecognized event of the name grammar.  \\nprivate static void NameSpeechRecognized(  \\n  object sender, SpeechRecognizedEventArgs e)  \\n{  \\n  Console.WriteLine(\\\"Grammar ({0}) recognized speech: {1}\\\",  \\n    e.Result.Grammar.Name, e.Result.Text);  \\n  \\n  try  \\n  {  \\n    // The name phrase starts after the first three words.  \\n    if (e.Result.Words.Count < 4)  \\n    {  \\n  \\n      // Add code to check for an alternate that contains the   \\nwildcard.  \\n      return;  \\n    }  \\n  \\n    RecognizedAudio audio = e.Result.Audio;  \\n    TimeSpan start = e.Result.Words[3].AudioPosition;  \\n    TimeSpan duration = audio.Duration - start;  \\n  \\n    // Add code to verify and persist the audio.  \\n    string path = @\\\"C:\\\\temp\\\\nameAudio.wav\\\";  \\n    using (Stream outputStream = new FileStream(path, FileMode.Create))  \\n    {  \\n      RecognizedAudio nameAudio = audio.GetRange(start, duration);  \\n      nameAudio.WriteToWaveStream(outputStream);  \\n      outputStream.Close();  \\n    }  \\n  \\n    Thread testThread =  \\n      new Thread(new ParameterizedThreadStart(TestAudio));  \\n    testThread.Start(path);  \\n  }  \\n  catch (Exception ex)  \\n  {  \\n    Console.WriteLine(\\\"Exception thrown while processing audio:\\\");  \\n    Console.WriteLine(ex.ToString());  \\n  }  \\n}  \\n  \\n// Use the speech synthesizer to play back the .wav file  \\n// that was created in the SpeechRecognized event handler.  \\n  \\nprivate static void TestAudio(object item)  \\n{  \\n  string path = item as string;  \\n  if (path != null && File.Exists(path))  \\n  {  \\n    SpeechSynthesizer synthesizer = new SpeechSynthesizer();  \\n    PromptBuilder builder = new PromptBuilder();  \\n    builder.AppendText(\\\"Hello\\\");  \\n    builder.AppendAudio(path);  \\n    synthesizer.Speak(builder);  \\n  }  \\n}  \\n```\"\n  syntax:\n    content: public void WriteToWaveStream (System.IO.Stream outputStream);\n    parameters:\n    - id: outputStream\n      type: System.IO.Stream\n      description: The stream that will receive the audio data.\n  overload: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream*\n  exceptions: []\n  platform:\n  - net462\nreferences:\n- uid: System.Object\n  isExternal: false\n  name: System.Object\n- uid: System.ArgumentOutOfRangeException\n  isExternal: true\n  name: System.ArgumentOutOfRangeException\n- uid: System.InvalidOperationException\n  isExternal: true\n  name: System.InvalidOperationException\n- uid: System.Speech.Recognition.RecognizedAudio.AudioPosition\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: AudioPosition\n  nameWithType: RecognizedAudio.AudioPosition\n  fullName: System.Speech.Recognition.RecognizedAudio.AudioPosition\n- uid: System.TimeSpan\n  parent: System\n  isExternal: true\n  name: TimeSpan\n  nameWithType: TimeSpan\n  fullName: System.TimeSpan\n- uid: System.Speech.Recognition.RecognizedAudio.Duration\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: Duration\n  nameWithType: RecognizedAudio.Duration\n  fullName: System.Speech.Recognition.RecognizedAudio.Duration\n- uid: System.Speech.Recognition.RecognizedAudio.Format\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: Format\n  nameWithType: RecognizedAudio.Format\n  fullName: System.Speech.Recognition.RecognizedAudio.Format\n- uid: System.Speech.AudioFormat.SpeechAudioFormatInfo\n  parent: System.Speech.AudioFormat\n  isExternal: false\n  name: SpeechAudioFormatInfo\n  nameWithType: SpeechAudioFormatInfo\n  fullName: System.Speech.AudioFormat.SpeechAudioFormatInfo\n- uid: System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: GetRange(TimeSpan,TimeSpan)\n  nameWithType: RecognizedAudio.GetRange(TimeSpan,TimeSpan)\n  fullName: System.Speech.Recognition.RecognizedAudio.GetRange(TimeSpan,TimeSpan)\n- uid: System.Speech.Recognition.RecognizedAudio\n  parent: System.Speech.Recognition\n  isExternal: false\n  name: RecognizedAudio\n  nameWithType: RecognizedAudio\n  fullName: System.Speech.Recognition.RecognizedAudio\n- uid: System.Speech.Recognition.RecognizedAudio.StartTime\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: StartTime\n  nameWithType: RecognizedAudio.StartTime\n  fullName: System.Speech.Recognition.RecognizedAudio.StartTime\n- uid: System.DateTime\n  parent: System\n  isExternal: true\n  name: DateTime\n  nameWithType: DateTime\n  fullName: System.DateTime\n- uid: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: WriteToAudioStream(Stream)\n  nameWithType: RecognizedAudio.WriteToAudioStream(Stream)\n  fullName: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(Stream)\n- uid: System.IO.Stream\n  parent: System.IO\n  isExternal: true\n  name: Stream\n  nameWithType: Stream\n  fullName: System.IO.Stream\n- uid: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: WriteToWaveStream(Stream)\n  nameWithType: RecognizedAudio.WriteToWaveStream(Stream)\n  fullName: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(Stream)\n- uid: System.Speech.Recognition.RecognizedAudio.AudioPosition*\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: AudioPosition\n  nameWithType: RecognizedAudio.AudioPosition\n- uid: System.Speech.Recognition.RecognizedAudio.Duration*\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: Duration\n  nameWithType: RecognizedAudio.Duration\n- uid: System.Speech.Recognition.RecognizedAudio.Format*\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: Format\n  nameWithType: RecognizedAudio.Format\n- uid: System.Speech.Recognition.RecognizedAudio.GetRange*\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: GetRange\n  nameWithType: RecognizedAudio.GetRange\n- uid: System.Speech.Recognition.RecognizedAudio.StartTime*\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: StartTime\n  nameWithType: RecognizedAudio.StartTime\n- uid: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream*\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: WriteToAudioStream\n  nameWithType: RecognizedAudio.WriteToAudioStream\n- uid: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream*\n  parent: System.Speech.Recognition.RecognizedAudio\n  isExternal: false\n  name: WriteToWaveStream\n  nameWithType: RecognizedAudio.WriteToWaveStream\n"}